"""
Chat Service - ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ë ˆì´ì–´

Phase 3.2: chat.pyì—ì„œ ì¶”ì¶œí•œ ê²€ì¦ëœ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
ê¸°ì¡´ ì½”ë“œ ê¸°ë°˜: app/api/chat.pyì˜ í•µì‹¬ í•¨ìˆ˜ë“¤

âš ï¸ ì£¼ì˜: ì´ ì½”ë“œëŠ” ê¸°ì¡´ ê²€ì¦ëœ ë¡œì§ì„ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.

## Service Layerì˜ ì—­í• 
- ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ë§Œ ë‹´ë‹¹ (HTTP ìš”ì²­/ì‘ë‹µê³¼ ë¶„ë¦¬)
- ëª¨ë“ˆ ì˜ì¡´ì„± ì£¼ì…ì„ í†µí•œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ì„± í™•ë³´
- RAG íŒŒì´í”„ë¼ì¸, ì„¸ì…˜ ì²˜ë¦¬, í†µê³„ ê´€ë¦¬ ë“± í•µì‹¬ ê¸°ëŠ¥ ì œê³µ
"""

import time
import uuid
from collections.abc import AsyncGenerator
from datetime import datetime
from typing import Any

from ...lib.cost_tracker import CostTracker
from ...lib.errors import ErrorCode, SessionError
from ...lib.logger import get_logger
from ...lib.metrics import PerformanceMetrics
from ...lib.types import RAGResultDict, SessionInfoDict, SessionResult, StatsDict
from .rag_pipeline import RAGPipeline

# LangSmith íŠ¸ë ˆì´ì‹± import
try:
    from langsmith import traceable

    LANGSMITH_AVAILABLE = True
except ImportError:
    LANGSMITH_AVAILABLE = False

    def traceable(*args, **kwargs):  # type: ignore[no-redef]
        def decorator(func):
            return func

        return decorator


logger = get_logger(__name__)


class ChatService:
    """
    ì±„íŒ… ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì„œë¹„ìŠ¤

    ì—­í• :
    - RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    - ì„¸ì…˜ ê´€ë¦¬
    - í†µê³„ ìˆ˜ì§‘
    - ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬

    ê¸°ì¡´ ì½”ë“œ ê¸°ë°˜: app/api/chat.pyì˜ í•¨ìˆ˜ë“¤ì„ í´ë˜ìŠ¤ë¡œ ì¬êµ¬ì„±
    """

    def __init__(self, modules: dict[str, Any], config: dict[str, Any]):
        """
        Args:
            modules: ì• í”Œë¦¬ì¼€ì´ì…˜ ëª¨ë“ˆ ë”•ì…”ë„ˆë¦¬ (DI)
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.modules = modules
        self.config = config

        # í†µê³„ ì •ë³´
        self.stats = {
            "total_chats": 0,
            "total_tokens": 0,
            "average_latency": 0.0,
            "error_rate": 0.0,
            "errors": 0,
        }

        # RAGPipeline ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì˜ì¡´ì„± ì£¼ì…)
        self.rag_pipeline = RAGPipeline(
            config=config,
            query_router=modules.get("query_router"),
            query_expansion=modules.get("query_expansion"),
            retrieval_module=modules.get("retrieval"),
            generation_module=modules.get("generation"),
            session_module=modules.get("session"),
            self_rag_module=modules.get("self_rag"),  # âœ… Self-RAG ëª¨ë“ˆ ì£¼ì…
            extract_topic_func=self.extract_topic,
            circuit_breaker_factory=modules.get(
                "circuit_breaker_factory"
            ),  # âœ… Circuit Breaker Factory ì£¼ì…
            cost_tracker=modules.get("cost_tracker") or CostTracker(),  # âœ… ë¹„ìš© ì¶”ì ê¸° ì£¼ì…
            performance_metrics=modules.get("performance_metrics")
            or PerformanceMetrics(),  # âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì£¼ì…
            sql_search_service=modules.get(
                "sql_search_service"
            ),  # âœ… SQL Search Service ì£¼ì… (Phase 3)
        )

        logger.info("ChatService ì´ˆê¸°í™” ì™„ë£Œ (RAGPipeline + Self-RAG + SQL Search í¬í•¨)")

    async def handle_session(
        self, session_id: str | None, context: dict[str, Any]
    ) -> SessionResult:
        """
        ì„¸ì…˜ ì²˜ë¦¬ - ê¸°ì¡´ ì„¸ì…˜ ê²€ì¦ ë˜ëŠ” ìƒˆ ì„¸ì…˜ ìƒì„±

        ê¸°ì¡´ ì½”ë“œ: chat.pyì˜ handle_session() í•¨ìˆ˜ (L235-298)

        Args:
            session_id: ìš”ì²­ëœ ì„¸ì…˜ ID (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
            context: ìš”ì²­ ì»¨í…ìŠ¤íŠ¸ (IP, User-Agent ë“±)

        Returns:
            ì„¸ì…˜ ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            session_module = self.modules.get("session")
            if not session_module:
                return {"success": False, "message": "Session module not available"}

            logger.debug(f"ğŸ” ì„¸ì…˜ ìš”ì²­ - ìš”ì²­ë°›ì€ session_id: {session_id}")

            if session_id:
                # ê¸°ì¡´ ì„¸ì…˜ ì¡°íšŒ
                logger.debug(f"ê¸°ì¡´ ì„¸ì…˜ ì¡°íšŒ ì‹œë„: {session_id}")
                session_result = await session_module.get_session(session_id, context)

                if session_result.get("is_valid"):
                    logger.debug(f"âœ… ì„¸ì…˜ ìœ íš¨í•¨ - session_id: {session_id}")
                    return {
                        "success": True,
                        "session_id": session_id,
                        "is_new": False,
                        "validation_result": session_result,
                    }
                else:
                    logger.warning(
                        f"ì„¸ì…˜ ë§Œë£Œ/ì—†ìŒ: {session_id}, "
                        f"ì´ìœ : {session_result.get('reason', 'unknown')}"
                    )

            # ìƒˆ ì„¸ì…˜ ìƒì„±
            logger.debug(f"ìƒˆ ì„¸ì…˜ ìƒì„± ì¤‘... (ê¸°ì¡´ ì„¸ì…˜: {session_id})")
            new_session = await session_module.create_session(
                {"metadata": context}, session_id=session_id
            )
            new_session_id = new_session["session_id"]

            logger.debug(f"âœ… ìƒˆ ì„¸ì…˜ ìƒì„± ì™„ë£Œ - session_id: {new_session_id}")

            return {
                "success": True,
                "session_id": new_session_id,
                "is_new": True,
                "message": "ìƒˆ ëŒ€í™” ì„¸ì…˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            }

        except KeyError as e:
            # ì„¸ì…˜ ëª¨ë“ˆ ì´ˆê¸°í™” ì•ˆ ë¨ ë˜ëŠ” í•„ìˆ˜ í‚¤ ëˆ„ë½
            logger.error(f"Session handling error - missing key: {e}", exc_info=True)
            raise SessionError(
                message="ì„¸ì…˜ ëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.",
                error_code=ErrorCode.SESSION_MODULE_NOT_AVAILABLE,
                context={"missing_key": str(e)},
                original_error=e,
            ) from e
        except Exception as e:
            # ì˜ˆìƒì¹˜ ëª»í•œ ì„¸ì…˜ ì²˜ë¦¬ ì—ëŸ¬
            logger.error(f"Session handling error: {e}", exc_info=True)
            raise SessionError(
                message="ì„¸ì…˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                error_code=ErrorCode.SESSION_CREATE_FAILED,
                context={"session_id": session_id, "context": context},
                original_error=e,
            ) from e

    def extract_topic(self, message: str) -> str:
        """
        í† í”½ ì¶”ì¶œ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)

        ê¸°ì¡´ ì½”ë“œ: chat.pyì˜ extract_topic() í•¨ìˆ˜ (L301-329)
        """
        # ì•ˆì „í•œ ë©”ì‹œì§€ ì²˜ë¦¬
        if isinstance(message, list):
            message = " ".join(str(item) for item in message)
        elif not isinstance(message, str):
            message = str(message)

        if not message:
            return "general"

        keywords = {
            "search": ["ê²€ìƒ‰", "ì°¾ê¸°", "ì°¾ì•„", "ê²€ìƒ‰í•´"],
            "document": ["ë¬¸ì„œ", "íŒŒì¼", "ìë£Œ", "ë°ì´í„°"],
            "help": ["ë„ì›€", "ë„ì™€", "ì„¤ëª…", "ì•Œë ¤"],
            "technical": ["ê¸°ìˆ ", "ê°œë°œ", "ì½”ë“œ", "í”„ë¡œê·¸ë˜ë°"],
            "general": ["ì¼ë°˜", "ê¸°ë³¸", "ì†Œê°œ", "ê°œìš”"],
        }

        try:
            lower_message = message.lower()

            for topic, words in keywords.items():
                if any(word in lower_message for word in words):
                    return topic

            return "general"
        except Exception:
            return "general"

    @traceable(
        name="RAGPipeline",
        tags=["chat", "rag", "pipeline"],
        metadata={"module": "chat_service", "version": "3.0.0"},
    )
    async def execute_rag_pipeline(
        self, message: str, session_id: str, options: dict[str, Any] | None = None
    ) -> RAGResultDict:
        """
        RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Phase 2 ê°œì„ : 150ì¤„ ë¸”ë™ë°•ìŠ¤ â†’ RAGPipeline.execute() ë‹¨ì¼ í˜¸ì¶œ
        - 8ê°œ ë…ë¦½ ë‹¨ê³„ë¡œ ë¶„í•´ëœ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
        - ë‹¨ê³„ë³„ ì„±ëŠ¥ ì¶”ì  (PipelineTracker)
        - Circuit Breaker, Graceful Degradation íŒ¨í„´ ì ìš©

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            options: ì¶”ê°€ ì˜µì…˜ (limit, min_score, top_n ë“±)

        Returns:
            RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼:
            {
                "answer": str,
                "sources": List[Source],
                "tokens_used": int,
                "topic": str,
                "processing_time": float,
                "search_results": int,
                "ranked_results": int,
                "model_info": Dict[str, Any],
                "routing_metadata": Optional[Dict[str, Any]],
                "performance_metrics": Dict[str, Any]  # NEW: PipelineTracker ë©”íŠ¸ë¦­
            }
        """
        logger.debug(
            "RAG Pipeline Starting (Phase 2 Refactored)",
            message_preview=message[:50],
            session_id=session_id,
        )

        # RAGPipeline.execute() ë‹¨ì¼ í˜¸ì¶œ (8ë‹¨ê³„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜)
        return await self.rag_pipeline.execute(
            message=message, session_id=session_id, options=options
        )

    async def add_conversation_to_session(
        self, session_id: str, user_message: str, assistant_answer: str, metadata: dict[str, Any]
    ) -> None:
        """
        ì„¸ì…˜ì— ëŒ€í™” ê¸°ë¡ ì¶”ê°€

        Args:
            session_id: ì„¸ì…˜ ID
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            assistant_answer: ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        """
        session_module = self.modules.get("session")
        if session_module:
            logger.debug(f"ëŒ€í™” ì¶”ê°€: session_id={session_id}")
            await session_module.add_conversation(
                session_id, user_message, assistant_answer, metadata
            )

    def update_stats(self, data: dict[str, Any]) -> None:
        """
        í†µê³„ ì—…ë°ì´íŠ¸

        ê¸°ì¡´ ì½”ë“œ: chat.pyì˜ update_stats() í•¨ìˆ˜ (L161-179)
        """
        self.stats["total_chats"] += 1

        if data.get("success"):
            if data.get("tokens_used"):
                self.stats["total_tokens"] += data["tokens_used"]

            if data.get("latency"):
                current_avg = self.stats["average_latency"]
                chat_count = self.stats["total_chats"]
                self.stats["average_latency"] = (
                    current_avg * (chat_count - 1) + data["latency"]
                ) / chat_count
        else:
            self.stats["errors"] += 1
            self.stats["error_rate"] = (self.stats["errors"] / self.stats["total_chats"]) * 100

    def get_stats(self) -> StatsDict:
        """í˜„ì¬ í†µê³„ ë°˜í™˜"""
        return self.stats.copy()  # type: ignore[return-value]

    async def get_session_info(self, session_id: str) -> SessionInfoDict:
        """
        ì„¸ì…˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ

        Returns:
            ì„¸ì…˜ ì •ë³´ ë”•ì…”ë„ˆë¦¬ (message_count, tokens_used, processing_time ë“±)
        """
        session_module = self.modules.get("session")
        if not session_module:
            raise Exception("Session module not available")

        # ì„¸ì…˜ ì¡´ì¬ í™•ì¸
        session_result = await session_module.get_session(session_id, {})
        if not session_result.get("is_valid"):
            raise Exception("Session not found")

        # ì±„íŒ… íˆìŠ¤í† ë¦¬ì—ì„œ í†µê³„ ì¶”ì¶œ
        history = await session_module.get_chat_history(session_id)
        messages = history.get("messages", [])

        # í†µê³„ ê³„ì‚°
        message_count = len(messages)
        total_tokens = 0
        total_processing_time = 0
        latest_model_info = None

        for message in messages:
            if message.get("type") == "assistant":
                if "tokens_used" in message:
                    total_tokens += message["tokens_used"]
                if "processing_time" in message:
                    total_processing_time += message["processing_time"]
                if "model_info" in message:
                    latest_model_info = message["model_info"]

        return {
            "session_id": session_id,
            "message_count": message_count,
            "tokens_used": total_tokens,
            "processing_time": total_processing_time,
            "model_info": latest_model_info,
            "timestamp": datetime.now().isoformat(),
        }

    async def stream_rag_pipeline(
        self, message: str, session_id: str | None, options: dict[str, Any] | None = None
    ) -> AsyncGenerator[dict[str, Any], None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        ì„¸ì…˜ ì²˜ë¦¬, ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„, ë¬¸ì„œ ê²€ìƒ‰, ë¦¬ë­í‚¹ì€ ë¹„ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³ ,
        ë‹µë³€ ìƒì„± ë‹¨ê³„ì—ì„œë§Œ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²­í¬ë¥¼ yieldí•©ë‹ˆë‹¤.

        ì´ë²¤íŠ¸ íƒ€ì…:
        - metadata: ê²€ìƒ‰ ê²°ê³¼ ë©”íƒ€ë°ì´í„° (ì„¸ì…˜ ID, ë¬¸ì„œ ìˆ˜, ì†ŒìŠ¤ ë“±)
        - chunk: LLM ì‘ë‹µ í…ìŠ¤íŠ¸ ì²­í¬ (data, chunk_index)
        - done: ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì´ë²¤íŠ¸ (session_id, total_chunks)
        - error: ì—ëŸ¬ ì´ë²¤íŠ¸ (error_code, message)

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
            options: ì¶”ê°€ ì˜µì…˜ (temperature, max_tokens, model ë“±)

        Yields:
            dict: ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ë”•ì…”ë„ˆë¦¬

        Example:
            async for event in chat_service.stream_rag_pipeline(message, session_id):
                if event["event"] == "chunk":
                    print(event["data"], end="", flush=True)
        """
        options = options or {}
        start_time = time.time()
        chunk_index = 0
        final_session_id = session_id

        try:
            # 1. ì„¸ì…˜ ì²˜ë¦¬ (ë¹„ìŠ¤íŠ¸ë¦¬ë°)
            session_module = self.modules.get("session")

            if session_module:
                if session_id:
                    # ê¸°ì¡´ ì„¸ì…˜ ê²€ì¦
                    session_result = await session_module.get_session(session_id, {})
                    if not session_result.get("is_valid"):
                        # ì„¸ì…˜ì´ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                        new_session = await session_module.create_session(
                            {"metadata": {}}, session_id=session_id
                        )
                        final_session_id = new_session["session_id"]
                        logger.debug(f"ìŠ¤íŠ¸ë¦¬ë°: ìƒˆ ì„¸ì…˜ ìƒì„± - {final_session_id}")
                else:
                    # ì„¸ì…˜ ID ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                    new_session = await session_module.create_session({"metadata": {}})
                    final_session_id = new_session["session_id"]
                    logger.debug(f"ìŠ¤íŠ¸ë¦¬ë°: ìƒˆ ì„¸ì…˜ ìƒì„± - {final_session_id}")

                # 2. ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ (ë¹„ìŠ¤íŠ¸ë¦¬ë°)
                session_context = await session_module.get_context_string(final_session_id)
            else:
                session_context = ""
                if not final_session_id:
                    final_session_id = str(uuid.uuid4())

            # 3. ë¬¸ì„œ ê²€ìƒ‰ (ë¹„ìŠ¤íŠ¸ë¦¬ë°)
            retrieval_module = self.modules.get("retrieval")
            search_results = []

            if retrieval_module:
                try:
                    search_results = await retrieval_module.search(message, {
                        "limit": options.get("limit", 8),
                        "min_score": options.get("min_score", 0.05),
                    })
                    logger.debug(f"ìŠ¤íŠ¸ë¦¬ë°: ê²€ìƒ‰ ì™„ë£Œ - {len(search_results)}ê°œ ë¬¸ì„œ")
                except Exception as e:
                    logger.warning(f"ìŠ¤íŠ¸ë¦¬ë°: ê²€ìƒ‰ ì‹¤íŒ¨ - {e}")

            # 4. ë¦¬ë­í‚¹ (ë¹„ìŠ¤íŠ¸ë¦¬ë°)
            reranked_documents = search_results  # ê¸°ë³¸ê°’: ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼
            reranking_applied = False

            if search_results:
                reranking_config = self.config.get("reranking", {})
                retrieval_config = self.config.get("retrieval", {})
                reranking_enabled = reranking_config.get("enabled", False) or retrieval_config.get(
                    "enable_reranking", False
                )

                if reranking_enabled:
                    retrieval_module = self.modules.get("retrieval")
                    if retrieval_module and hasattr(retrieval_module, "rerank"):
                        try:
                            rerank_top_n = options.get("top_n", reranking_config.get("top_n", 8))
                            reranked_documents = await retrieval_module.rerank(
                                query=message,
                                results=search_results,
                                top_n=rerank_top_n,
                            )

                            # min_score í•„í„°ë§
                            min_score = reranking_config.get("min_score", 0.05)
                            if min_score > 0:
                                reranked_documents = [
                                    doc
                                    for doc in reranked_documents
                                    if (hasattr(doc, "score") and doc.score >= min_score)
                                    or (hasattr(doc, "metadata") and doc.metadata.get("score", 0) >= min_score)
                                ]

                            reranking_applied = True
                            logger.debug(
                                f"ìŠ¤íŠ¸ë¦¬ë°: ë¦¬ë­í‚¹ ì™„ë£Œ - {len(reranked_documents)}ê°œ ë¬¸ì„œ"
                            )
                        except Exception as e:
                            logger.warning(f"ìŠ¤íŠ¸ë¦¬ë°: ë¦¬ë­í‚¹ ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš© - {e}")
                            reranked_documents = search_results
                    else:
                        logger.debug("ìŠ¤íŠ¸ë¦¬ë°: ë¦¬ë­í‚¹ ëª¨ë“ˆ ì—†ìŒ, ì›ë³¸ ì‚¬ìš©")
                else:
                    logger.debug("ìŠ¤íŠ¸ë¦¬ë°: ë¦¬ë­í‚¹ ë¹„í™œì„±í™”, ì›ë³¸ ì‚¬ìš©")

            # 5. ë©”íƒ€ë°ì´í„° ì´ë²¤íŠ¸ ì „ì†¡
            metadata_event = {
                "event": "metadata",
                "data": {
                    "session_id": final_session_id,
                    "search_results": len(search_results),
                    "ranked_results": len(reranked_documents),
                    "reranking_applied": reranking_applied,
                    "message_id": str(uuid.uuid4()),
                    "timestamp": datetime.now().isoformat(),
                },
            }
            yield metadata_event

            # 6. ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
            generation_module = self.modules.get("generation")

            if generation_module and hasattr(generation_module, "stream_answer"):
                # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ ì¤€ë¹„ (ë¦¬ë­í‚¹ëœ ë¬¸ì„œ ì‚¬ìš©)
                context_documents = reranked_documents if reranked_documents else []

                # ìƒì„± ì˜µì…˜ êµ¬ì„±
                generation_options = {
                    **options,
                    "session_context": session_context,
                }

                # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
                try:
                    async for text_chunk in generation_module.stream_answer(
                        query=message,
                        context_documents=context_documents,
                        options=generation_options,
                    ):
                        chunk_event = {
                            "event": "chunk",
                            "data": text_chunk,
                            "chunk_index": chunk_index,
                        }
                        yield chunk_event
                        chunk_index += 1

                except Exception as e:
                    logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
                    yield {
                        "event": "error",
                        "error_code": ErrorCode.GENERATION_REQUEST_FAILED.value,
                        "message": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                    }
                    return
            else:
                # ìƒì„± ëª¨ë“ˆì´ ì—†ê±°ë‚˜ ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš°
                logger.warning("ìŠ¤íŠ¸ë¦¬ë°: ìƒì„± ëª¨ë“ˆ ì—†ìŒ ë˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì›")
                yield {
                    "event": "chunk",
                    "data": "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "chunk_index": 0,
                }
                chunk_index = 1

            # 7. ì™„ë£Œ ì´ë²¤íŠ¸ ì „ì†¡
            processing_time = time.time() - start_time
            done_event = {
                "event": "done",
                "data": {
                    "session_id": final_session_id,
                    "total_chunks": chunk_index,
                    "processing_time": processing_time,
                    "tokens_used": 0,  # ìŠ¤íŠ¸ë¦¬ë°ì—ì„œëŠ” ì •í™•í•œ í† í° ê³„ì‚° ì–´ë ¤ì›€
                },
            }
            yield done_event

            logger.info(
                f"ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: session_id={final_session_id}, "
                f"chunks={chunk_index}, time={processing_time:.2f}s"
            )

        except Exception as e:
            # ì—ëŸ¬ ì´ë²¤íŠ¸ ì „ì†¡
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì—ëŸ¬: {e}", exc_info=True)
            yield {
                "event": "error",
                "error_code": ErrorCode.INTERNAL_ERROR.value if hasattr(ErrorCode, "INTERNAL_ERROR") else "GEN-999",
                "message": f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            }
